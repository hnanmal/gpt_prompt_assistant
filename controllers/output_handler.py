# controllers/output_handler.py

from utils.ollama_client import ask_ollama_stream
from utils.keyword_utils import extract_keywords
from utils.file_matcher import find_related_files
from utils.context_builder import infer_project_context


def start_ollama_analysis(
    viewmodel, user_input, on_token_callback, on_complete_callback
):
    print("ğŸš€ start_ollama_analysis ì§„ì…")
    if not user_input:
        on_complete_callback("ìš”ì²­ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    model = viewmodel.get_current_model()
    if not model:
        on_complete_callback("[ì˜¤ë¥˜] í˜„ì¬ ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    result_accumulator = []

    def on_token(token):
        result_accumulator.append(token)
        on_token_callback(token)

    def on_done(*args, **kwargs):
        print("ğŸ”¥ on_done í˜¸ì¶œë¨")
        result = "".join(result_accumulator)
        viewmodel.set_last_ollama_result(result)
        on_complete_callback(result)

    # í•µì‹¬: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    full_prompt = viewmodel.build_stream_prompt(user_input)

    ask_ollama_stream(
        model=model,
        prompt=full_prompt,  # â¬…ï¸ í•µì‹¬ ìˆ˜ì •!
        on_token_callback=on_token,
        on_complete_callback=on_done,
        should_stop_callback=viewmodel.should_stop,
    )


# def start_ollama_analysis(
#     viewmodel, user_input, on_token_callback, on_complete_callback
# ):
#     print("ğŸš€ start_ollama_analysis ì§„ì…")  # â† ì´ê±° ì¶œë ¥ë˜ëŠ”ì§€ í™•ì¸
#     if not user_input:
#         on_complete_callback("ìš”ì²­ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")
#         return

#     model = viewmodel.get_current_model()
#     if not model:
#         on_complete_callback("[ì˜¤ë¥˜] í˜„ì¬ ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
#         return

#     result_accumulator = []

#     def on_token(token):
#         result_accumulator.append(token)
#         on_token_callback(token)

#     def on_done(*args, **kwargs):
#         print("ğŸ”¥ on_done í˜¸ì¶œë¨")
#         ollama_result = "".join(result_accumulator)
#         viewmodel.set_last_ollama_result(ollama_result)  # ğŸ”¥ ìŠ¤íŠ¸ë¦¼ ê²°ê³¼ ì €ì¥

#         keywords = extract_keywords(user_input)
#         related_files = find_related_files(viewmodel.context.code_root, keywords)
#         related_files_text = "\n".join(f"- {f}" for f in related_files[:5]) or "(ì—†ìŒ)"

#         if viewmodel.context.config_summary:
#             context_info = viewmodel.context.config_summary
#         else:
#             inferred = infer_project_context(viewmodel.context.project_path or ".")
#             context_info = "\n".join(f"- {k}: {v}" for k, v in inferred.items())

#         prompt_parts = [
#             f"### ğŸ”§ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸\n{context_info or '(ì—†ìŒ)'}",
#             f"### ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°\n{viewmodel.context.tree_structure or '(ì—†ìŒ)'}",
#             f"### ğŸ¤– Ollama ë¶„ì„ ê²°ê³¼\n{ollama_result}",
#             f"### ğŸ“‚ ê´€ë ¨ íŒŒì¼ ì¶”ì²œ (ë£° ê¸°ë°˜)\n{related_files_text}",
#             f"### ğŸ—£ï¸ ë‚´ ìš”ì²­:\n{user_input}",
#         ]
#         final_prompt = "\n\n".join(prompt_parts)

#         on_complete_callback(final_prompt)

#     ask_ollama_stream(
#         model=model,
#         prompt=f"""
# ë‹¤ìŒ ìš”ì²­ ë¬¸ì¥ì—ì„œ ê´€ë ¨ëœ ê¸°ëŠ¥, ì»´í¬ë„ŒíŠ¸, ëª¨ë“ˆ, íŒŒì¼ëª…ì„ ì¶”ë¡ í•´ì„œ ëª©ë¡ìœ¼ë¡œ ì•Œë ¤ì¤˜.
# ì—†ê±°ë‚˜ ëª¨ë¥´ê² ëŠ” ê±´ 'ëª¨ë¦„'ì´ë¼ê³  í•´ë„ ì¢‹ì•„.
# ìš”ì²­: "{user_input}"
# """.strip(),
#         on_token_callback=on_token,
#         on_complete_callback=on_done,
#     )
